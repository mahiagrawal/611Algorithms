\documentclass[11pt]{article}
\usepackage{fullpage} % better margins
\usepackage{amsthm,amssymb,amsmath} % useful math commands
\usepackage{graphicx} % figures
\usepackage{xfrac} % inline fractions
\usepackage{enumerate} % custom enum symbols
\usepackage{algpseudocode, algorithm} % for pseudocode

% Define some useful shortcuts
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\E}{\mathcal{E}}
\renewcommand{\O}{\mathcal{O}}
% if you want to use \alg and \opt in text, wrap them in math mode to make the spacing work
% e.g. We have shown that $\alg$ runs in $\O(n)$
\newcommand{\alg}{\textsc{alg}}
\newcommand{\opt}{\textsc{opt}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\title{Solutions for 611 Homework 2}

\author{Mahim Agarwal, Hanwen Xiong, and Jackson Warley}

\begin{document}

\maketitle

\section{Solution to Question 1}

Let $\M = (E, \I)$. For all $i \in [k]$, we have $|\emptyset \cap E_i| = 0 \leq 1$, so $\emptyset \in \I$.
To prove heredity, let $S \in \I$ and suppose $R \subseteq S$.
Then, if $i \in [k]$, we have $R \cap E_i \subseteq S \cap E_i$, implying $|R \cap E_i| \leq |S \cap E_i| \leq 1$.
To prove exchange, first note the following property: because the (nonempty) sets $E_1, \dots, E_k$ partition $E$ and each independent set intersects any $E_i$ in at most one element, any $S \in \I$ has nontrivial intersection with exactly $|S|$ of the sets $E_1, \dots, E_k$.
Suppose $A, B \in \I$ with $|A| < |B|$.
Then $B$ intersects $|B| > |A|$ of the sets $E_1, \dots, E_k$, so there must be some $i$ such that $E_i \cap A = \emptyset$ and $E_i \cap B = \{b\}$.
Then $A \cup \{b\} \in \I$, since $|(A \cup \{b\}) \cap E_i| = 1$, proving that $\M$ is a matroid.

\section{Solution to Question 2}

We will present the algorithm first, followed by the analysis.
\begin{algorithm}
\begin{algorithmic}
  \Function{alg}{$P = \{p_1, p_2, \dots, p_n\}$}
    \State $I \gets \{[p_1, p_1 + 1]\}$
    \For{$p \in P$ left to right}
      \State $[a, b] \gets \operatorname{last}(I)$
      \If{$p > b$}
      \State $I \gets I \cup \{[p, p + 1]\}$
      \EndIf
    \EndFor
  \EndFunction
\end{algorithmic}
\end{algorithm}

\noindent {\bf Claim 1:} $\alg$ runs in $\O(n)$ time and returns the smallest possible set of intervals covering all the points in $P$.
\begin{proof}
  Since $\alg$ terminates exactly when we have considered every point in $P$, and $P$ is finite, $\alg$ must terminate.
  When it does, $I$ will contain some number of intervals, greater than $1$.
  If $\alg$ ever encounters a point $p$ that is not covered by an interval already in $I$, it adds an interval containing $p$ to $I$.
  Thus, since every point is considered, all points are covered when $\alg$ terminates.

  Now, we must show that $P$ cannot be covered using fewer than $|I|$ (after $\alg$ has run to completion) unit-length intervals.
  Let $P' = \{p \in P \mid [p, p + 1] \in I\}$.
  Since every interval in $I$ begins with a point from $P$, $|P'| = |I|$.
  Furthermore, since $\alg$ only constructs a new interval when the nearest endpoint is further than $1$ away from the point under consideration, no two points in $P'$ are within $1$ of each other.
  Thus, any cover of $P'$ must include at least $|P'| = |I|$ intervals, and in particular the solution yielded by $\alg$ is optimal.

  Finally, $\alg$ considers each point in $P$ exactly once.
  The work of comparing the points and retrieving the most recently added interval requires only constant time, so considering a single point is $\O(1)$.
  Thus, $\alg$ has a total runtime of $\O(n)$.
\end{proof}




\section{Solution to Question 3}

\begin{enumerate}[\indent (1)]
    \item $(\Rightarrow)$ We prove the contrapositive.
      Let $S \subseteq J$ and suppose that there exists some $d \in [n]$ for which $S_d > d$.
      Then, by the pigeonhole principle, in any ordering of $S$ there must be some job $s_i$ with $i > d$ and $d_{s_i} < d$.
      Since we can idenfity an overdue job $s_i$ for any ordering of $S$, $S$ must not be good.

      $(\Leftarrow)$ Let $S \subseteq J$ with $S_d \leq d$ for all $d \in [n]$ and suppose toward a contradiction that $S$ is not good.
      Then in any ordering of $S$, there is some overdue job.
      Let $d + 1$ be the index of the first overdue job in $S$ and choose the ordering $S = \{s_1, \dots, s_k\}$ that maximizes $d$.
      Then $S$ is of the form $S = \{s_1, \dots, s_d, s_{d+1}, \dots, s_k\}$ where jobs $s_1$ through $s_{d}$ yield bonuses and $s_{d+1}$ does not.
      Suppose that for some $i \in [d]$, we have $d_{s_i} > d$.
      Then we could exchange $s_i$ with $s_{d+1}$, after which $s_{d+1}$ would yield a bonus, and, since $d_{s_i} > d$, so would $s_i$.
      However, this would contradict the maximality of $d$, since it would result in an ordering in which the first $d+1$ jobs yield bonuses.
      This means that for all $i \in [d]$, $d_{s_i} \leq d$, implying that $S_d \geq d$.
      But we also have $d_{s_{d+1}} \leq d$, since $s_{d+1}$ is overdue.
      Thus $S_d > d$, a contradiction, whereby $S$ must be good.

    \item To prove heredity, suppose that $A \in \I$.
      Let $A = \{a_1, \dots, a_k\}$ be a good ordering of $A$, and let $B \subseteq A$ be ordered identically up to removal of elements.
      Since $B$ is obtained by removing jobs from $A$, the index in $B$ of any job in $A \cap B$ is at most its index in $A$, so any job that yielded a bonus in $A$ still yields one in $B$.
      Thus, since $A$ was good, so is $B$, whence $B \in \I$ as desired.

      Now, to prove exchange, let $A, B \in \I$ with $|A| < |B|$.
      Suppose that $A = \{a_1, \dots, a_k\}$ and $B = \{b_1, \dots, b_j\}$.
      Let $A' = A \cup \{b_j\}$.
      Since $B \in \I$, we must have $d_{b_j} \geq j \geq k + 1$, therefore $b_j$ yields a bonus in $A'$.
      The other jobs in $A'$ have not moved from their positions in $A$, so they still yield bonuses.
      Thus $A' \in \I$, implying $(J, \I)$ has independence.

    \item Let $w(j) = b_j$ for all $j \in J$.
    Since $(J, \I)$ is a matroid, we know that we can apply the matroid greedy algorithm to find a maximum-weight independent set.
\end{enumerate}

\section{Solution to Question 4}

\subsection{Dynamic Approach}
\noindent {\bf Definition 4.1.1}: {\it Maximal Sequence Sum of $\{a_1, \dots, a_i\}$ is the optimal sequence sum when we're at the $a_i$ entry

\noindent {\bf Definition 4.1.2}: $S_i = ${\it the optimal sequence sum of $\{a_1, \dots, a_i\}$ including $a_i$

\noindent{\bf Algorithm:}

\begin{algorithm}
\begin{algorithmic}
  \State create an array $S$ of size $n+1$ and make the $0th$ entry $0$
  \State $maximum = 0, maximum\_index = 0$
  \For{$i \in [n]$}
    \State $S[i] = max\{S[i-1], 0\} + a_i$
    \If{$S[i] > maximum$}
      \State $maximum = S[i], maximum\_index = i$
    \EndIf
  \EndFor
  \State Scan from maximum\_index to its left until we hit a non-positive entry of S
  \State The entries scanned(not including the non-positive one) is the MSS of the whole sequence
\end{algorithmic}
\end{algorithm}

\begin{proof}
Whenever we go to the next number $a_i$ we either chose to or not to include it in the Maximal Sequence Sum so far at $a_{i-1}$. So we have this two cases: case 1: $a_i$ included; case 2: $a_i$ not included.

For case 1, we have to add $a_i$ to our Maximal Sequence Sum of $\{a_1, \dots, a_{i-1}\}$, which is $S_i$ by definition; For case 2, we know if $a_i$ is not included, then the Maximal Sequence Sum will be $0$ at $a_i$ because the sequence sum has to be sum of contiguous numbers. In this case, Maximal Sequence Sum is always $0$ so that we utilize this property and don't have to create another array.

Obviously the Maximal Sequence Sum at $a_i$ is the larger one of case 1 and case 2 as we only have this two cases. Therefore, at any $a_i$ we have this $2*n$($1*n$ in actual operation) table that tracks the Maximal Sequence Sum. The maximum value of this table then is the Maximum Sequence Sum. The sequence should start from the last number that's not included, at which the Maximal Sequence Sum is $0$. a Maximal Sequence Sum of $0$ suggests $S_i <= 0$. So it's reasonable for our algorithm to stop scanning at that entry and return the sequence.
\end{proof}

\noindent {\bf Time Complexity}: It takes $\O(n)$ to create a length-$(n+1)$ array and $\O(n)$ to traverse $n$ numbers. And we have to scan at most $n+1$ entries of array $S$ which also takes $\O(n)$ to conduct. Therefore, the time complexity is $\O(n)$.

\subsection{Divide-and-Conquer Approach}

\noindent{\bf Algorithm:}

\begin{algorithm}
\begin{algorithmic}
  \State 1.Divide the Sequence into equal halves, get two sub-MSS $MSS_l$ and $MSS_r$
  \State 2.Keep adding nubmers from the boder of $MSS_l$ and $MSS_r$ to its left until we hit the begining of $MSS_l$, record the maximum sum as $MSS_{bl}$
  \State 3.Keep adding nubmers from the boder of $MSS_l$ and $MSS_r$ to its right until we hit the end of $MSS_r$, record the maximum sum as $MSS_{br}$
  \State 4.return $max\{MSS_l, MSS_r, MSS_{bl}+MSS_{br}\}$
\end{algorithmic}
\end{algorithm}

\begin{proof}
When we merge the two sequences it's possible for us to get a even larger MSS than $MSS_l$ and $MSS_r$. And we can only do this by crossing the middle. Because if we can add more numbers from the two "sides" then we'd get a larger $MSS_l$ or $MSS_r$ which results in contradiction.

From above we can conclude that there're 4 cases:
\begin{itemize}
\item $MSS_l +$ some sequence to its right in the "middle"
\item $MSS_r +$ some sequence to its left in the "middle"
\item $MSS_l +$ "middle" sequence + $MSS_r$"
\item some sequence in the "middle" and is not right next to $MSS_l$ or $MSS_r$
\end{itemize}

It's obvious that our algorithm covers all the cases, and it chooses the maximum of $MSS_l$, $MSS_r$ and $MSS_{bl}+MSS_{br}$. So if there's a larger sequence crossing the middle we'll find and return that

\end{proof}

\noindent {\bf Time Complexity}: Every time we divide the problem into 2 subproblems of half size. And the merge step scan from the middle to the two sides which takes linear time. So we get $T(n) = T(n/2) + O(n)$. According to Master's Theorem, the total time complexity is $O(nlogn)$

\section{Solution to Question 5}



\section{Solution to Question 6}



\end{document}
