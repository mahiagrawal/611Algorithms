\documentclass[11pt]{article}
\usepackage{fullpage} % better margins
\usepackage{amsthm,amssymb,amsmath} % useful math commands
\usepackage{graphicx} % figures
\usepackage{xfrac} % inline fractions
\usepackage{enumerate} % custom enum symbols
\usepackage{algpseudocode, algorithm} % for pseudocode

% Define some useful shortcuts
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\E}{\mathcal{E}}
\renewcommand{\O}{\mathcal{O}}
% if you want to use \alg and \opt in text, wrap them in math mode to make the spacing work
% e.g. We have shown that $\alg$ runs in $\O(n)$
\newcommand{\alg}{\textsc{alg}}
\newcommand{\opt}{\textsc{opt}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\title{Solutions for 611 Homework 2}

\author{Mahim Agarwal, Hanwen Xiong, and Jackson Warley}

\begin{document}

\maketitle

\section{Solution to Question 1}

Let $\M = (E, \I)$. For all $i \in [k]$, we have $|\emptyset \cap E_i| = 0 \leq 1$, so $\emptyset \in \I$.
To prove heredity, let $S \in \I$ and suppose $R \subseteq S$.
Then, if $i \in [k]$, we have $R \cap E_i \subseteq S \cap E_i$, implying $|R \cap E_i| \leq |S \cap E_i| \leq 1$.
To prove exchange, first note the following property: because the (nonempty) sets $E_1, \dots, E_k$ partition $E$ and each independent set intersects any $E_i$ in at most one element, any $S \in \I$ has nontrivial intersection with exactly $|S|$ of the sets $E_1, \dots, E_k$.
Suppose $A, B \in \I$ with $|A| < |B|$.
Then $B$ intersects $|B| > |A|$ of the sets $E_1, \dots, E_k$, so there must be some $i$ such that $E_i \cap A = \emptyset$ and $E_i \cap B = \{b\}$.
Then $A \cup \{b\} \in \I$, since $|(A \cup \{b\}) \cap E_i| = 1$, proving that $\M$ is a matroid.

\section{Solution to Question 2}

We will present the algorithm first, followed by the analysis.
\begin{algorithm}
\begin{algorithmic}
  \Function{alg}{$P = \{p_1, p_2, \dots, p_n\}$}
    \State $I \gets \{[p_1, p_1 + 1]\}$
    \For{$p \in P$ left to right}
      \State $[a, b] \gets \operatorname{last}(I)$
      \If{$p > b$}
      \State $I \gets I \cup \{[p, p + 1]\}$
      \EndIf
    \EndFor
  \EndFunction
\end{algorithmic}
\end{algorithm}

\noindent {\bf Claim:} $\alg$ runs in $\O(n)$ time and returns the smallest possible set of intervals covering all the points in $P$.
\begin{proof}
  Since $\alg$ terminates exactly when we have considered every point in $P$, and $P$ is finite, $\alg$ must terminate.
  When it does, $I$ will contain some number of intervals, greater than $1$.
  If $\alg$ ever encounters a point $p$ that is not covered by an interval already in $I$, it adds an interval containing $p$ to $I$.
  Thus, since every point is considered, all points are covered when $\alg$ terminates.

  Now, we must show that $P$ cannot be covered using fewer than $|I|$ (after $\alg$ has run to completion) unit-length intervals.
  Let $P' = \{p \in P \mid [p, p + 1] \in I\}$.
  Since every interval in $I$ begins with a point from $P$, $|P'| = |I|$.
  Furthermore, since $\alg$ only constructs a new interval when the nearest endpoint is further than $1$ away from the point under consideration, no two points in $P'$ are within unit distance of each other.
  Thus, any cover of $P'$ by unit intervals must include at least $|P'| = |I|$ intervals, and in particular the solution yielded by $\alg$ is optimal.

  Finally, $\alg$ considers each point in $P$ exactly once.
  The work of comparing the points and retrieving the most recently added interval requires only constant time, so considering a single point is $\O(1)$.
  Thus, $\alg$ has a total runtime of $\O(n)$.
\end{proof}




\section{Solution to Question 3}

\begin{enumerate}[\indent (1)]
    \item $(\Rightarrow)$ We prove the contrapositive.
      Let $S \subseteq J$ and suppose that there exists some $d \in [n]$ for which $S_d > d$.
      Then, by the pigeonhole principle, in any ordering of $S$ there must be some job $s_i$ with $i > d$ and $d_{s_i} < d$.
      Since we can idenfity an overdue job $s_i$ for any ordering of $S$, $S$ must not be good.

      $(\Leftarrow)$ Let $S \subseteq J$ with $S_d \leq d$ for all $d \in [n]$ and suppose toward a contradiction that $S$ is not good.
      Then in any ordering of $S$, there is some overdue job.
      Let $d + 1$ be the index of the first overdue job in $S$ and choose the ordering $S = \{s_1, \dots, s_k\}$ that maximizes $d$.
      Then $S$ is of the form $S = \{s_1, \dots, s_d, s_{d+1}, \dots, s_k\}$ where jobs $s_1$ through $s_{d}$ yield bonuses and $s_{d+1}$ does not.
      Suppose that for some $i \in [d]$, we have $d_{s_i} > d$.
      Then we could exchange $s_i$ with $s_{d+1}$, after which $s_{d+1}$ would yield a bonus, and, since $d_{s_i} > d$, so would $s_i$.
      However, this would contradict the maximality of $d$, since it would result in an ordering in which the first $d+1$ jobs yield bonuses.
      This means that for all $i \in [d]$, $d_{s_i} \leq d$, implying that $S_d \geq d$.
      But we also have $d_{s_{d+1}} \leq d$, since $s_{d+1}$ is overdue.
      Thus $S_d > d$, a contradiction, whereby $S$ must be good.

    \item To prove heredity, suppose that $A \in \I$.
      Let $A = \{a_1, \dots, a_k\}$ be a good ordering of $A$, and let $B \subseteq A$ be ordered identically up to removal of elements.
      Since $B$ is obtained by removing jobs from $A$, the index in $B$ of any job in $A \cap B$ is at most its index in $A$, so any job that yielded a bonus in $A$ still yields one in $B$.
      Thus, since $A$ was good, so is $B$, whence $B \in \I$ as desired.

      Now, to prove exchange, let $A, B \in \I$ with $|A| < |B|$.
      Suppose that $A = \{a_1, \dots, a_k\}$ and $B = \{b_1, \dots, b_j\}$.
      Let $A' = A \cup \{b_j\}$.
      Since $B \in \I$, we must have $d_{b_j} \geq j \geq k + 1$, therefore $b_j$ yields a bonus in $A'$.
      The other jobs in $A'$ have not moved from their positions in $A$, so they still yield bonuses.
      Thus $A' \in \I$, implying $(J, \I)$ has independence.

    \item Let $w(j) = b_j$ for all $j \in J$.
    Since $(J, \I)$ is a matroid, we know that we can apply the matroid greedy algorithm to find a maximum-weight independent set.
    If $J$ is the set of jobs (somehow tied to their deadlines and bonuses so that both can be accessed in constant time), and $\textsc{mga}$ is the matroid greedy algorithm with the weight function $w(j) = b_j$, our algorithm is as follows:
    \begin{algorithm}
      \begin{algorithmic}
        \Function{alg}{$J$}
          \State call $\textsc{mga}(J)$ to find a maximum-weight good set $S$
          \State sort $S$ in non-decreasing order by deadline
          \State append $J \setminus S$ to $S$ in any order
          \State \Return the resulting list
        \EndFunction
      \end{algorithmic}
    \end{algorithm}
\end{enumerate}

\noindent {\bf Claim 1:} $\alg$ terminates in $\O(n \log n)$ time.
\begin{proof}
  The total runtime of $\alg$ is the runtime of $\textsc{mga}$ plus the $\O(n \log n)$ required to sort $S$ and append $J \setminus S$ to it (the latter operation is at most linear in $n$).
  $\textsc{mga}$ first requires $\O(n \log n)$ to sort the jobs by bonus.
  Next, it considers each job once, choosing whether to include it or not based on whether it maintains the independence of the accumulator set.
  By Part (1), we can check whether the inclusion of a job maintains independence in constant time; we merely keep track of $S_d$ as elements are added, and check only the new candidate element to determine its inclusion.
  Thus, the remaining work done by $\textsc{mga}$ requires only $\O(n)$ time, so the total runtime of $\textsc{mga}$, and therefore also of $\alg$, is $\O(n \log n)$.
\end{proof}

\begin{lemma}\label{lem:earliest}
  If $S$ is a good set, the earliest-deadline-first ordering completes every job in $S$.
\end{lemma}

\begin{proof}
  Let $S = \{s_1, \dots, s_k\}$ be any ordering that completes every job in $S$.
  If this ordering is not the earliest-deadline-first ordering, there is some pair of jobs $s_i$ and $s_j$ so that $i < j$ and $d_{s_i} > d_{s_j}$.
  Exchanging the positions of these two jobs does not cause any jobs to become overdue, since $s_j$ is completed earlier than it previously was, $s_i$ is due after $s_j$ (which was not overdue in its original position), and the other jobs are unaffected.
  Since we can perform these exchanges until no more such pairs exist, we can transform any witness ordering of a good set into the earliest-deadline-first ordering without causing any jobs to become overdue.
\end{proof}

\begin{lemma}\label{lem:max-good-set}
  If $S$ is a maximum-weight good set, then any ordering of $J$ that completes every job in $S$ yields the maximum bonus amount.
\end{lemma}
\begin{proof}
  Let $J = \{j_1, \dots, j_n\}$ be an ordering of $J$ that completes every job in $S$.
  Suppose toward a contradiction that there is some other ordering $J' = \{j'_1, \dots, j'_n\}$ of the same jobs that yields a greater sum of bonuses than $J$.
  Let $Q = \{j' \in J' \mid j' \textnormal{ is completed on time in } J'\}$.
  Then $Q$ is a good set, as witnessed by the ordering given by removing $J' \setminus Q$ from $J'$ and reindexing its remaining elements in the order they originally appeared.
  But, since the removed elements were not completed on time, they did not contribute to the sum of bonuses yielded by $J'$.
  Thus, $Q$ is a good set yielding the same sum of bonuses as $J'$, and in particular it yields more than $J$, which completed every item in the maximum-yield good set.
  Thus $Q$ contradicts the maximality of $S$, and therefore there must be no ordering of $J'$ that yields more in total than $J$.
\end{proof}

\noindent {\bf Claim 2:} $\alg$ returns an ordering that yields the maximum possible bonus.
\begin{proof}
  By construction, the ordering returned by $\alg$ includes the maximum-weight good set, in earliest-deadline-first order.
  By Lemma~\ref{lem:earliest}, every job in this maximum-yield set is completed.
  By Lemma~\ref{lem:max-good-set}, this implies that $\alg$ gives the ordering that maximizes the total bonus yield.
\end{proof}

\section{Solution to Question 4}

\subsection{Dynamic Approach}
\noindent {\bf Definition 4.1.1}: {\it Maximal Sequence Sum of $\{a_1, \dots, a_i\}$ is the optimal sequence sum when we're at the $a_i$ entry

\noindent {\bf Definition 4.1.2}: $S_i = ${\it the optimal sequence sum of $\{a_1, \dots, a_i\}$ including $a_i$

\noindent{\bf Algorithm:}

\begin{algorithm}
\begin{algorithmic}
  \State create an array $S$ of size $n+1$ and make the $0th$ entry $0$
  \State $maximum = 0, maximum\_index = 0$
  \For{$i \in [n]$}
    \State $S[i] = max\{S[i-1], 0\} + a_i$
    \If{$S[i] > maximum$}
      \State $maximum = S[i], maximum\_index = i$
    \EndIf
  \EndFor
  \State Scan from maximum\_index to its left until we hit a non-positive entry of S
  \State The entries scanned(not including the non-positive one) is the MSS of the whole sequence
\end{algorithmic}
\end{algorithm}

\begin{proof}
Whenever we go to the next number $a_i$ we either chose to or not to include it in the Maximal Sequence Sum so far at $a_{i-1}$. So we have this two cases: case 1: $a_i$ included; case 2: $a_i$ not included.

For case 1, we have to add $a_i$ to our Maximal Sequence Sum of $\{a_1, \dots, a_{i-1}\}$, which is $S_i$ by definition; For case 2, we know if $a_i$ is not included, then the Maximal Sequence Sum will be $0$ at $a_i$ because the sequence sum has to be sum of contiguous numbers. In this case, Maximal Sequence Sum is always $0$ so that we utilize this property and don't have to create another array.

Obviously the Maximal Sequence Sum at $a_i$ is the larger one of case 1 and case 2 as we only have this two cases. Therefore, at any $a_i$ we have this $2*n$($1*n$ in actual operation) table that tracks the Maximal Sequence Sum. The maximum value of this table then is the Maximum Sequence Sum. The sequence should start from the last number that's not included, at which the Maximal Sequence Sum is $0$. a Maximal Sequence Sum of $0$ suggests $S_i <= 0$. So it's reasonable for our algorithm to stop scanning at that entry and return the sequence.
\end{proof}

\noindent {\bf Time Complexity}: It takes $\O(n)$ to create a length-$(n+1)$ array and $\O(n)$ to traverse $n$ numbers. And we have to scan at most $n+1$ entries of array $S$ which also takes $\O(n)$ to conduct. Therefore, the time complexity is $\O(n)$.

\subsection{Divide-and-Conquer Approach}

\noindent{\bf Algorithm:}

\begin{algorithm}
\begin{algorithmic}
  \State 1.Divide the Sequence into equal halves, get two sub-MSS $MSS_l$ and $MSS_r$
  \State 2.Keep adding nubmers from the boder of $MSS_l$ and $MSS_r$ to its left until we hit the begining of $MSS_l$, record the maximum sum as $MSS_{bl}$
  \State 3.Keep adding nubmers from the boder of $MSS_l$ and $MSS_r$ to its right until we hit the end of $MSS_r$, record the maximum sum as $MSS_{br}$
  \State 4.return $max\{MSS_l, MSS_r, MSS_{bl}+MSS_{br}\}$
\end{algorithmic}
\end{algorithm}

\begin{proof}
When we merge the two sequences it's possible for us to get a even larger MSS than $MSS_l$ and $MSS_r$. And we can only do this by crossing the middle. Because if we can add more numbers from the two ``sides" then we'd get a larger $MSS_l$ or $MSS_r$ which results in contradiction.

From above we can conclude that there're 4 cases:
\begin{itemize}
\item $MSS_l +$ some sequence to its right in the ``middle"
\item $MSS_r +$ some sequence to its left in the ``middle"
\item $MSS_l +$ ``middle" sequence + $MSS_r$
\item some sequence in the ``middle" and is not right next to $MSS_l$ or $MSS_r$
\end{itemize}

It's obvious that our algorithm covers all the cases, and it chooses the maximum of $MSS_l$, $MSS_r$ and $MSS_{bl}+MSS_{br}$. So if there's a larger sequence crossing the middle we'll find and return that

\end{proof}

\noindent {\bf Time Complexity}: Every time we divide the problem into 2 subproblems of half size. And the merge step scan from the middle to the two sides which takes linear time. So we get $T(n) = T(n/2) + O(n)$. According to Master's Theorem, the total time complexity is $O(nlogn)$

\section{Solution to Question 5}

Let us suppose, input array $P = \{P_1, \dots, P_n\}$ and we need to find Longest decreasing subsequence (LDS). 
We will use the concept of dynamic programming where we will find the LDS for $\{P_i, \dots, P_n\}$ 
given that we know LDS for $\{P_{i+1}, \dots, P_n\}$

\noindent Here is our proposed algorithm:

\noindent{\bf Algorithm:} 

\noindent Consider three arrays as follows:

\begin{itemize}
\item input array $P = \{P_1, \dots, P_n\}$
\item dynamic programming array $D = \{D_1, \dots, D_n\}$ with all elements initialized to 1 and
where $D_i$ is the longest decresing sub-sequence length from $P_i$ to $P_n$ {\bf WITH $P_i$ INCLUDED}
\item predecessor array $S = \{S_1, \dots, S_n\}$  with all elements initialized to -1 and
where $S_i$ is the index of the element that is the predecessor to $P_i$ in the longest
decreasing subsequence from $P_i$ to $P_n$
\end{itemize}

\noindent Now, algorithm is as follows:

\begin{algorithm}
\begin{algorithmic}
  \For{$i \in \{n-1, n-2, \dots, 1\}$}
    \For{$j \in \{n, n-1, \dots, i+1\}$}
      \If{$P_i > P_j$ and $D_i > 1 + D_j$}
        \State$D_i += 1, S_i = j$
      \EndIf
    \EndFor
  \EndFor
  \State Find the the index $k$ of the maximum element in array D
  \State append $P_k$ to the result sub-sequence, find index of its predecessor from $S_k$ to append the element to final list. Keep appending the predecessors until $S_x = -1$, at which point we will return since it is the last element of resulting LDS.
\end{algorithmic}
\end{algorithm}

\noindent {\bf Time complexity: } We do the outer loop and inner loop roughly n times, which suggests complexity of $\O(n^2)$. Traversing the array D and S both take $\O(n)$. So we have $T(n) = \O(n^2) + \O(n) = \O(n^2)$

\noindent {\bf Accuracy explanation: }

{\bf proof of concept:}
We know that for every iteration on $P_i$, $D_{i+1}$ to $D_n$ contains length of LDS including that particular $D_j$.
So, when we traverse from $P_n$ to $P_{i+1}$, for any element $P_j$, if LDS is $\{P_j, P_l, P_o\}$ and if $P_i > P_j$, then $P_i > P_j > P_l > P_o$.
Thus LDS at $P_i$ is $\{P_i, P_j, P_l, P_o\}$ which has length 1 greater than LDS length at $P_j$. 
Also, since we only update the $D_i$ when new length is greater than the existing one, we will find the sequence with the longest LDS at $D_i$ with element at index i included.
Once we reach the max possible length of LDS, we can produce the final sequence by traversing through the predecessor array $S$, since for every time we update $D_i$, $S_i$ contains the next element of the LDS starting from $P_i$
 
{\bf proof of termination:}
Since, we have intialized $S$ array with -1, for every $S_x$ the value is either -1 or an index from $P$ array (1 to n).

Case 1: $S_i = -1$; This means that this element has no element to its right which is smaller than $P_i$. 
So, this must be the smallest element of any LDS which includes it and  thus our algorithm should terminate here which it does.

Case 2: $S_i \in \{1,2, \dots, n\}$; Then we recurse to  its next smaller element and if it's the smallest we terminate. Else we go to next smaller.

\section{Solution to Question 6}

\subsection{Solution H}

\noindent{\bf Algorithm:}

\begin{algorithm}
\begin{algorithmic}
  \State Create a list $c[n]$ which stores $(c_i+i-1, i)$ in its $i-th$ entry. Index starts from 1
  \State Sort the list by the first element of each tuple, get the sorted array $c_{sorted}[n]$
  \State create an empty list $L$, a variable $i = 0$
  \For{$c' \in c_{sorted}[n]$}
    \If{the second element of $c' > i$}
      \State $L$.append$(c')$, update $i = c'$ 
    \EndIf
  \EndFor
\end{algorithmic}
\end{algorithm}

\noindent{\bf Definition 6.H.1}: {\it $Cost(i, j) = c_j + \sum_{j \in [n]} u_j$}. This is the cost of storing file on server $S_i$ plus the cost of some user(s) accessing file on server $S_j$

\begin{proof}
  Suppose a user wants to access server $S_i$. The user can either find the file on $S_i$ or on some $S_j$ where $j > i$. The $Cost(i, j) = c_j + (j-i)$.
  Consider the condition where we have to find the file elsewhere. Then for the cost to be cheapest, we have to satisfy $Cost(i, j) < Cost(i, i)$, i.e. $c_j + (j-i) < c_i$ and $Cost(i, j)$ is the minimum $\forall j > i$. It's not hard to find for all access between $S_i$ and $S_j$, the cheapest cost will always be achieved by saving the file on $S_j$. Because if not, there exists $Cost(m, n) < Cost(m, j)$ where $i< m \leq n < j \Rightarrow c_n + (n-m) < c_j + (j-m) \Rightarrow c_n + (n-m) -i + i < c_j + (j-m) -i + i \Rightarrow c_n + (n-i) < c_j + (j-i) \Rightarrow Cost(i, n) < Cost(i, j)$. This contradicts to our assumption that $Cost(i, j)$ is the minimum

For our algorithm, we first sort the list $c[n]$. The element in it is well designed so that $\forall Cost(i, j) < Cost(i, i), c[j] < c[i]$. Therefore, everytime we get an element out of $c_{sorted}[n]$, we don't consider any server with a subscript less than the current subscirpt. Finally we may get an $L = \{l_1, l_2, \dots, l_m\}$. From the proof above we know access to $S_{l_i}$ can always find the file at its corresponding interval. Since the user won't access from right to left we are assured the cost is always cheapest.
\end{proof}

\noindent {\bf Time Complexity}: Creating and intializing the list costs $\O(n)$. Sorting the list costs $\O(nlogn)$. Traverse the sorted list costs $\O(n)$. So the overall complexity of this algorithm is $O(nlogn)$

\subsection{Solution M}
Given n servers with their copying cost, we want to find the optimal number of servers and the optimum combination of servers
such that our cost function is minimized. \newline
{\bf We will use the concept of dynamic programming similar to how we used in the previous question.} \newline
For any server $S_i \in S_1, \dots , S_n$, we will find the optimal soultion for sub-list of servers of $S_i$
to $S_n$ {\bf with $S_i$ included}  using already found optimal solution at a server later than i and before n. \newline
Here is how it works:  \newline
for $i \in (1,n)$ after having chosen $S_i$ let us suppose in the optimal solution for $S_i$, the next server chosen is $S_k$
where $k \in (i+1 , n)$ and it is given that we know optimal solutions for all k.  \newline
So, optimum cost function will be: \newline

$opt\_cost(i) = C_i + \min_{\forall k \in (i+1,n)} (\frac{(k-i)(k-i-1)}{2} + opt\_Cost(k))$

Once, we have found $opt\_cost(i) \forall i \in (1,n)$ we can traverse the whole opt\_cost array to find the minimum cost.

\noindent{\bf Algorithm:} Consider two additional arrays , apart from $S$  and $C$: \newline
minimum cost array {\bf $opt\_cost$} : length n with all elements initialized to $\infty$ and where \newline
opt\_cost(i) gives us the minimium cost of storing the file on $S_i$ to $S_n$ servers with $S_i$ chosen. \newline
and optimal combination {\bf $opt\_servers$} array \newline 
where $opt\_servers$(i) gives us the combination of selected servers for optimal solution including $S_i$.
\newline
Now,
\begin{enumerate}
  \item For i = n to 1:

  \item {\bf Base case} \newline
  if i==n: opt\_cost(i) = $c_n$ = 0
  end if

else: {\bf Dynamic case:} \newline
	  for j = i+1 to n: \newline
		  min = $\infty$ \newline
		  $min\_index = j$ \newline
		  if ($\frac{(j-i)(j-i-1)}{2} + $opt\_Cost(j)$) < min:$ \newline
			  min = ($\frac{(j-i)(j-i-1)}{2} + opt\_Cost(j)$) \newline
			  min\_index = j  \newline
      end if \newline
    end for \newline

$opt\_Cost(i)$ = $(\frac{(min\_index-i)(min\_index-i-1)}{2} + opt\_Cost(min\_index))$ \newline
$opt\_servers(i)$ = $S_i$ + $opt\_servers(min\_index)$ ({\bf append strings}) \newline
end else

\item Once we are done with the iterations, traverse the whole array $opt\_cost$ to find the index(k) of the min cost.
\item output opt\_cost(k) and opt\_servers(k)
 \end{enumerate}

{\bf Complexity Analysis:} \newline
determining entire array of $opt\_cost$ using looping on i and j: O($n^2$) \newline
traversing whole array to find the minimum cost: O(n)

So time complexity = O($n^2$) + O(n) = O($n^2$)

\end{document}
