\documentclass[11pt]{article}
\usepackage{fullpage} % better margins
\usepackage{amsthm,amssymb,amsmath} % useful math commands
\usepackage{graphicx} % figures
\usepackage{xfrac} % inline fractions
\usepackage{enumerate} % custom enum symbols
\usepackage{algpseudocode,algorithm} % for pseudocode

% Define some useful shortcuts
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\E}{\mathcal{E}}
\renewcommand{\O}{\mathcal{O}}
% if you want to use \alg and \opt in text, wrap them in math mode to make the spacing work
% e.g. We have shown that $\alg$ runs in $\O(n)$
\newcommand{\alg}{\textsc{alg}}
\newcommand{\opt}{\textsc{opt}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\title{Solutions for 611 Homework 5}

\author{Mahim Agarwal, Hanwen Xiong, and Jackson Warley}

\begin{document}

\maketitle

\section{Solution to Question 1}

\begin{enumerate}
    \item Choose k sets from $\mathcal{B}$ such that $|{\cup_{i=1}^k C_i}|$ is maximized \newline
    The greedy approach will give us the required approximation. The algorithm is as follows:
    \begin{algorithm}
  \begin{algorithmic}
    \Function{alg}{$[n] , \mathcal{B}, k$}
      \State $\mathcal{C} \leftarrow \Phi$
      \State uncovered elements, $E \leftarrow [n]$
      \While{$|\mathcal{C}| < k$}
          \State Determine the set $B_i$ which contains the maximum number of uncovered points from $E$
          \State $\mathcal{C} \leftarrow \mathcal{C} + \{B_i\}$
          \State update $E$ to delete all the elements that are present in $B_i$, i.e. $E \leftarrow E - (E \cap B_i)$
      \EndWhile
      \State \Return $\mathcal{C}$
    \EndFunction
  \end{algorithmic}
\end{algorithm}\\
Let us assume that the optimal solution covered k sets such that $|{\cup_{i=1}^k C_i}| = opt $ number of elements. We know that $opt \leq n$ \\
Since the optimal solution uses k sets, by pigeon-hole principle, there must be at least one set that contains equal to or more than 1/k 
fraction of the elements. Now, since the algorithm chooses the set that covers the most elements, so the chosen set must cover at least 
that many new elements. Therefore, after the first iteration of the algorithm, there are at most $opt(1 - 1/k)$ elements left to cover. \\
Now, we might have chosen the set which belongs to optimal solution(in which case we are left with k-1 more optimal sets) or we might 
have chosen some other set (in which case we still have k more optimal sets). Taking the worst case scenario of not choosing the optimal 
set, we have k optimal sets remaining.\\
Going by the same principle used earlier, in the next iteration, we choose the set that covers the most elements remaining and after the 
second iteration, there are at most $(opt(1 - 1/k))(1 - 1/k)$ elements remaining. \\
So, after k iterations, there are at most $opt(1 - 1/k)^k$ elements left. \\
{\bf I think the little problem with this proof is that we can't say if we hit worst case scenario in each iteration, the final result will also be worst case.} \\
In other words, after k iterations, we have covered at least $opt - opt(1 - 1/k)^k$ elements. \\
Thus the approximation ratio of this algorithm is: \\
$\frac{C_{opt}}{C_{alg}} = \frac{opt}{opt - opt(1- 1/k)^k} $
$\Longrightarrow$ approximation ratio $= 1/(1 - (1 - 1/k)^k)$

    \item Find smallest set $D \subseteq [n]$ such that $B \cap D \neq \Phi $ \\
    The greedy approach will give us the required approximation. The algorithm is as follows:   
    \begin{algorithm}
  \begin{algorithmic}
    \Function{alg}{$[n] , \mathcal{B}$}
      \State $\mathcal{D} \leftarrow \Phi$
      \State disjoint subsets, $\mathcal{B}$ (given)
      \For{i from 1 to m}
        \If{$B_i \cap \mathcal{D} = \Phi$}
            \State $\mathcal{D} \leftarrow \mathcal{D} + B_i$
         \EndIf
        \EndFor
      \State \Return $\mathcal{D}$
    \EndFunction
  \end{algorithmic}
\end{algorithm}\\
In words, starting from the first subset, keep adding the subset to our solution if it is a disjoint subset. \\
Now, let us suppose the algorithm chose k subsets for creating the output $\mathcal{D}$. Also, we know that $b = max_i |B_i|$. \\
So, maximum number of elements output set $\mathcal{D}$ can contain = $b*k$\\
Thus, $minD_{alg} \leq b*k$ \\
Also, since algorithm only chooses a subset if it is disjoint, that means the k subsets that we chose are all disjoint from each other. 
Thus, the optimal solution should contain at least one element each from all these k subsets, otherwise we will have $\mathcal{D}_{opt} 
\cap B_x = \Phi$ for one or more of $B_i s$\\
Thus, $minD_{opt} \geq k$ \\
Hence, approximation ratio, $\frac{minD_{alg}}{minD_{opt}} \leq \frac{b*k}{k}$ \\
$\Longrightarrow$ approximation ratio = b
\end{enumerate}

\newpage

\noindent {\bf Q1P1 Proof:}
\begin{proof}
	\noindent Assume the optimal solution has $|\cup_{i=1}^{k}C_i| = opt$ number of elements, obviously $opt \leq n$.
	Suppose we have some elements that we can cover, we argue that in the next step we can cover at least $\frac{1}{k}$ fraction of the uncovered elements in the optimal sets.
	To see why, suppose we've picked $j$ number of sets and there're $k-j$ number of sets left.
	If the next step we can only cover less than $\frac{1}{k}$ fraction of the uncoverd elements in the optimal sets, then the returned result $\mathcal{C}$ will always satisfy $|\mathcal{C}| < |\cup_{i=1}^{j}C_i| + (k-j) * \frac{|optimal~sets/\{C_1, C_2, \dots, C_j\}|}{k}$.
	Now, assume we're picking in a way that opimizes $\mathcal{C}$ in our return result.
	Then, the righthand side of the inequality will be strictly less than $opt$ since $(k-j) < k$.
	This means we can never pick sets from $\mathcal{B}$ in a way that reaches $opt$.
	Contradicted to our assumption, so our claim is true.
	Therefore, after $k$ iterations, we have covered at least $opt - opt(1 - 1/k)^k$ elements, which gives us a performance ratio of $1/(1 - (1 - 1/k)^k)$.
\end{proof}

\newpage
\section{Solution to Question 2}
The greedy approach will give us the required approximation. The algorithm is as follows:
    \begin{algorithm}
  \begin{algorithmic}
    \Function{alg}{$G = (V, E)$}
      \State $S \leftarrow \Phi$
      \While{G is not empty}
          \State Determine the node v of minimum degree in G
          \State $S \leftarrow S \cup \{v\}$
          \State Remove v and its neighbors(vertices connected through an edge) from G
      \EndWhile
      \State \Return S
    \EndFunction
  \end{algorithmic}
\end{algorithm}\\
At the end of the algorithm, the cardinality of independent set output is $|S|$ and all the vertices that were deleted because of choosing their neighbors in S are $|V \backslash S|$ \\
Now, consider a vertex u in S. Since we chose u and the maximum degree it can have is d, the maximum number of vertices that were deleted because of choosing u are d. \\
So, there are at most d vertices in $V \backslash S$ for every vertex in S. Note that it can be lower than this as two non-neighboring vertices chosen in S might have common neighbor. \\
$\Longrightarrow |V \backslash S| \leq d|S| $ \\
Also, since each vertex is either chosen or deleted and there is no vertex left unoperated in G at the end, $|S| + |V \backslash S| = |V| = n$ \\
$\Longrightarrow n - |S| \leq d|S|$ \\
$\Longrightarrow |S| \geq \frac{n}{d+1}$ \\
Since, the maximum size of an independent set in a graph can be n, the approximation of this algorithm is: \\
$\frac{\frac{n}{d+1}}{n} = \frac{1}{d+1}$
\newpage
\section{Solution to Question 3}
\begin{enumerate}
    \item Min-Squares is NP-Complete
    \begin{proof}\\
  \textbf{Problem is in NP:} \\
   If we are given r number of sets of positive integers then we can easily verify whether the given sum is less than L or not in polynomial time. We can traverse through each set once, calculating their individual sums and then calculate the sum of squares of these individual sums to check whether the final sum is less than L.This all can be done in $\mathcal{O}(n)$ time.
   Thus, this problem is in NP. \\
  \textbf{Problem is NP-Hard:} \\
  asdsad 
   \end{proof}
   \item Approximation algorithm for minimizing $\sum_{i=1}^r (\sum_{x \in S_i} x)^2$ \\
   We will try to partition given set such that each partition has their individual sums as close as possible to each other. The algorithm is as follows:
   \begin{algorithm}
  \begin{algorithmic}
    \Function{alg}{$x_1, x_2, ...., x_n, r$}
      \State $S_1 \leftarrow \Phi, S_2 \leftarrow \Phi,.....,S_n \leftarrow \Phi$
      \State Sort the given set of integers in decreasing order of their values.
      \State Put first(largest) r elements in the sorted array to each of the sets $S_1, S_2, ...., S_r$ such that each set contains exactly one element.
      \For{i from r+1 to n}
        \State Find the subset $S_k$ with the minimum individual sum among all subsets from $S_1$ to $S_r$. 
        \State $S_k \leftarrow S_k \cup {x_i}$
        \EndFor
      \State \Return $S_1, S_2, ...., S_r$
    \EndFunction
    \end{algorithmic}
    \end{algorithm}\\
    To calculate the approximation ratio of this algorithm , consider the following claims:
    
    \textbf{Claim 1:} All the elements that are added from r+1 to n have value less than $\sum_{i=1}^n x_i/r$\\
    i.e. $\forall k \in \{r+1,r+2,...,n\}$, $ x_k < \sum_{i=1}^n x_i/r$
    \begin{proof} by contradiction \\
    Let $(r+1)^{th}$ element is greater than $\sum_{i=1}^n x_i/r$\\
    If any element after index r is greater than $\sum_{i=1}^n x_i/r$, then all the elements from 1 to r are at least $\sum_{i=1}^n x_i/r$ as the set is arranged in decreasing order.\\
    So, r+1 elements have value greater than or equal to $\sum_{i=1}^n x_i/r$\\
    $\Longrightarrow \sum_{i=1}^{r+1} x_i \geq (r+1)\sum_{i=1}^n x_i/r > \sum_{i=1}^n x_i$ \\
    Since, the sum cannot be greater than sum of all elements, our assumption is wrong.\\
    Thus the claim that no element from r+1 to n in the sorted set is greater than $\sum_{i=1}^n x_i/r$ is valid.
    \end{proof}
    \textbf{Claim 2:} The individual sum of any subset returned from the above algorithm can be at most $2* \sum_{i=1}^n x_i/r$ \\ 
    i.e. $\forall k \in \{1,2,...,r\}$, $\sum_{x \in S_k} x \leq 2* \sum_{i=1}^n x_i/r$
    \begin{proof}
     Let among the r subsets that algorithm returned, $S_k$ had the maximum individual sum. Also, assume that the last element we added to $S_k$ was $x_j$.\\
     According to the algorithm, an element is added to a subset only if the individual sum of that subset is lowest.Thus, \\
     $\forall {i \neq k}$, $\sum_{x \in S_i} x \geq \sum_{x \in S_k} x - x_j$ \\
     $\Longrightarrow$ Total sum of all the individual subsets at that point, $\sum_{i=1}^r (\sum_{x \in S_i} x) \geq r(\sum_{x \in S_k} x - x_j)$ \\
     Also, $r(\sum_{x \in S_k} x - x_j) \leq \sum_{i=1}^n x_i$ (Sum of all elements)\\
     From \textbf{Claim 1} we know that $x_j \leq \frac{\sum_{i=1}^n x_i}{r}$ \\
     Hence, $\sum_{x \in S_k} x \leq 2*\frac{\sum_{i=1}^n x_i}{r}$ \\
     Thus, the claim is valid.
    \end{proof}
    Thus, maximum possible value of $\sum_{i=1}^r (\sum_{x \in S_i} x)^2$ = $\sum_{i=1}^r (2*\sum_{i=1}^n x_i/r)^2$ \\
    $\Longrightarrow minSum_{alg} \leq 4*r*(\sum_{i=1}^n x_i/r)^2$ \\
    Also, we know (also the given hint) that $\sum_{i=1}^r (\sum_{x \in S_i} x)^2$ $\geq r*(\sum_{i=1}^n x_i/r)^2$ \\
    $\Longrightarrow$ Approximation ratio $= minSum_{alg}/minSum_{optimal}$ \\
    $\Longrightarrow$ Approximation ratio $=  \frac{4*r*(\sum_{i=1}^n x_i/r)^2}{r*(\sum_{i=1}^n x_i/r)^2}$\\
    $\Longrightarrow$ Approximation ratio = 4
    
    \textbf{Note that} if an element itself is greater than $2*\frac{\sum_{i=1}^n x_i}{r}$ then we dont have any choice but to make a separate subset of only this element and choose other subsets so as to minimize the sum. In other words, it will be similar to running the algorithm for creating r-1 subsets with n-1 elements(excluding the one with large value). And this won't change the approximation much. Also, there can only be r/2-1 number of such elements otherwise we can never create r subsets.
\end{enumerate}
\newpage
\section{Solution to Question 4}
\begin{proof}\\
  \textbf{Problem is in NP:} \\
   If we are given a set of vertices which constitute the dominating set then we can easily verify whether it is a dominating set or not in polynomial time. We can check that the size of the given set is k by traversing it once in polynomial time. Also, we can traverse all vertices of the given graph, and check whether they themselves or their neighboring vertex lie in the given dominating set or not.
   Thus, this problem is in NP. \\
  \textbf{Problem is NP-Hard:} \\
   To show this we will reduce the NP-complete vertex cover problem to the given problem. \\
   Given an undirected graph G, create a graph H by replacing each edge of G with a clique of size 3(a triangle of 3 edges connecting 3 vertices, where 2 vertices are of original graph while one vertex is added new).\\ In other words, create a graph $H(V', E')$, \\ where $V' = V \cup V_e$ for $V_e = \{v_{e_i}|e_i \in E\}$ \\
   and $E' = E \cup E_e$ for $E_e = \{(v_{e_i},v_k)(v_{e_i},v_l)|e_i = (v_k, v_l) \in E\}$ \\
   \textbf{Claim 1:} If G has a vertex cover of size k then graph H has a dominating set of size k. \\
   For every edge $(u, v)$ in graph G, at least one of u or v has to be in the vertex cover by the definition. Let us suppose u is in vertex cover. Now, consider the clique formed by corresponding vertices(u, v, $v_{e_{u,v}}$) in graph H. For each vertex of this clique, either it itself is in the vertex cover (in our case u) or it's neighbor is in the vertex cover(neighbor of v is u and neighbor of $v_{e_{u,v}}$ is also u). Thus, if G has a vertex cover of size k then H has a dominating set of size k constituting same set of vertices as in vertex cover.\\  
   \textbf{Claim 2:} If H has a dominating set of size k, then graph G has a vertex cover of size k.\\
   Without loss of generality we can assume that the dominating set only has vertices from graph G and does not contain new vertices added, $v_{e_i}$. This is so because even if it did contain a vertex $v_{e_i}$ then, we can easily replace it with either of the vertex u or v in the clique of $v_{e_i}$ and it still will be a dominating set of graph H without any increase in its size. Now, this dominating set of vertices form a vertex cover for graph G. This is so because for every edge $e_i$ inclined to $v_{e_i}$, at least one of the two neighbors u or v has to be in the dominating set. Thus, for every edge, $e \in E$ of graph G, we have a vertex(u or v) in dominating set. This will make the dominating set a vertex cover for graph G.
   \end{proof}
   Since, the problem is both in NP and NP-Hard, it is NP-Complete.

\newpage
\section{Solution to Question 5} 
(1)Assume we have a input graph $G = (V, E)$ and need to solve its dominating set.
We do a transformation like following: Transform each vertex of $G$ to some student in the minimum max-min distance problem.
For each vertex pair, if there's an edge between the two vertices then set the distance of the two corresponding students to 1 mile.
Otherwise set it to 2 miles. \\

\noindent{\bf Claim: }{\it There exists a dominating set of size $t$ of $G$ iff there exists a subset $C$ of size $t$ such that $max-dist(C) < 2$ in the converted minimum max-min distance problem}.

\begin{proof}
	First we need to show that the converted instance of minimum max-min distance problem satisfies the inequality $d_{ij} + d_{jk} \geq d_{ik}$.
	According to our transformation, a student can go to another student with distance of either $1$ or $2$ miles.
	if the student first go to some other student(s), then it takes the student at least $2$ miles to get to the destination.
	Thus the inequality is established.
	Then we prove the iff statement below:

	$\Rightarrow$: If there exists a dominating set of size $t$ of $G$, then by nominating the students that are correspondingly vertices in the dominating set to $C$ we can satisfy $max-dist(C) < 2$. 
	According to the definition of dominating set we know a vertex is either in it or incident to an edge whose another endpoint is in it.
	Therefore, in the converted instance a student is either a TA or can get to a TA with distance of $1$ mile.
	Since $1$ mile is the minimum distance for a student to get to any other students, $max-dist(C)$ can only be $1$.

	$\Leftarrow$: In the constructed students placement, a student can either go to another student with distance of $1$ or $2$ miles.
	Since we're looking for a set of $C$ which satisfies $max-dist(C) < 2$, a student will never go to a TA with distance larger than or equal to $2$ miles.
	The distance in the instance can only be integral, so $max-dist(C)$ has to be $1$.
	This suggests that a student is either a TA or can go to a TA with distance of $1$ mile.
	Therefore, suppose we can find such a set $C$ of size $t$.
	We know every vertex outside of set $C$ can reach some corresponding vertex in set $C$ directly.
	This means the vertices in set $C$ is a dominating set of size $t$.
\end{proof}

\noindent Note that we can construct the student placement in time propotional to the number of vertices $n$.
	Converting all the vertices to students takes time $\O(n)$.
	Add distance to each student pair takes time $\O(n^2)$.
	Thus, the reduction from dominating set to the question in this part is in polynomial time.\\

\noindent (2)The approximation ratio the algorithm is 2.

\begin{proof}
	Assume the returned $C$ isn't the optimal solution, $C'$ is the optimal solution.

	\noindent First we argue that there're at least $t+1$ students that live at least $r$ miles away from each other, where $r$ is $max-dist(C)$.
	This is true because at least one student other than those in $C$ is $r$ miles away from all the students in $C$ according to the algorithm.
	There're at least one student who needs to walk to students in $C$ since it makes no sense to nominate all the students.
	That case, the denominator of approximation ratio is $0$.
	The walking distance will be at most $r$ miles.
	Plus the $t$ students in $C$ are also at least $r$ miles away from each other since we pick students greedily.
	We have at least $t+1$ students that are at least $r$ miles away from each other.

	\noindent Since $C' \neq C$ and $|C'| = |C| = t$, there're at least $2$ students in the above mentioned $t+1$ students that are not in $C'$.
	This is because if there're only $1$ student who's in the above mentioned $t+1$ students while not in $C'$, then the only one left student will be at least $r$ miles away from all the $t$ students in $C'$ which makes no improvement at all.
	Let's assume the $2$ students are student $i$ and $j$.
	For any student $k$ in $C'$, consider the distance between $k$ and $i$, $j$.
	We have $d_{ij} \geq r$ as argued above.
	And $d_{ik} + d_{kj} \geq d_{ij} \geq r$. Then one of $d_{ik} (d_{ki})$ and $d_{kj}$ will have to be at least $\frac{r}{2}$.
	Therefore, $max-dist(C') \geq \frac{r}{2}$.
	Which gives us the approximation ratio of 2.
\end{proof}
\end{document}
