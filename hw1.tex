\documentclass[11pt]{article}
\usepackage{fullpage} % better margins
\usepackage{amsthm,amssymb,amsmath} % useful math commands
\usepackage{graphicx} % figures
\usepackage{xfrac} % inline fractions
\usepackage{enumitem} % custom enum symbols
\usepackage{algpseudocode}

% Define some useful shortcuts
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\E}{\mathcal{E}}
\renewcommand{\O}{\mathcal{O}}
% if you want to use \alg and \opt in text, wrap them in math mode to make the spacing work
% e.g. We have shown that $\alg$ runs in $\O(n)$
\newcommand{\alg}{\textsc{alg}}
\newcommand{\opt}{\textsc{opt}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\title{Solutions for 611 Homework 1}

\author{Mahim Agarwal, Hanwen Xiong, and Jackson Warley}

\begin{document}

\maketitle

\section{Solution to Question 1}
Let $A = \{a_1, \dots, a_n\}$ be the array that we need to count the inversion for.
Consider the following algorithm $\alg$ for determining whether $A$ is boring or not:
\begin{enumerate}
  \item Divide the array into 2 equal parts: $A_l = \{a_1, \dots, a_{n/2}\}$ and $A_r = \{a_{n/2+1}, \dots, a_n\}$
  \item Recursively count the number of inversions in each half by calling the algorithm on $A_l$ and $A_r$
  \item Merge the two halves into a sorted whole(increasing order) and count the number of inversions across
the two halves(from $A_l$ to $A_r$).
\item Return the sum of three counts. \newline
 \end{enumerate}
{\bf Procedure:}\newline
CountInversion(\{$a_1$, \dots, $a_n$\}) \newline
if n==1 return count = 0 \newline
\newline
else:

 $count_l$ = CountInversion(\{$a_1$, \dots,$a_{n/2}$\})

 $count_r$ = CountInversion(\{$a_{n/2+1}$, \dots, $a_n$\})

return  $count_l$ + $count_r$ + Merge(sorted $A_l$, sorted $A_r$)

{\bf Merge Step:} \newline
Let us suppose we have two sorted halves $A$ and $B$ in increasing order of elements. Then do the following: \newline
Start with the first element of $A$ and $B$ and move towards right. \newline
Compare $a_i$ and $b_j$ \newline
If $a_i$ \textless  $b_j$, count = count + 0. No inversion of $a_i$. Move on to next element of $A$. \newline
If $a_i$ \textgreater $b_j$, count = count + all elements remaining in A. Move on to the next element of $A$ and next  element of $B$ \newline
Keep appending the smaller of the two elements $a_i$ and $b_j$ to the sorted list $C$. \newline
{\bf Accuracy explanation:}\newline

We will use the following lemma to prove the correctness of $\alg$.

\begin{lemma}\label{inversion-count-lemma}
If $A = \{a_1, \dots, a_{n/2}\}$ and $B = \{b_1, \dots, b_{n/2}\}$ are two sorted arrays in increasing order where
$A$ is the left half while $B$ is the right half, then \newline
if $a_i$ \textless $b_j$: inversion count for $a_i$ beyond this point = 0 \newline
while if $a_i$ \textgreater $b_j$: inversion count for this $b_j$ = rest of the elements in $A$. ($\frac{n}{2}$ - i)
\end{lemma}

\begin{proof}
Since $A$ and $B$ are sorted in increasing order: \newline
$a_1$ \textless $a_2$ \textless  $a_3$ \textless \dots \textless $a_{n/2}$ \newline
and  $b_1$ \textless $b_2$ \textless  $b_3$ \textless \dots \textless $b_{n/2}$ \newline

Since we are dividing the array into two halves, all elements in the left half will have index less than all elements in the right half.
Thus, for inversion, element in the left half should be greater than element in the right half. \newline

So, if $a_i$ \textgreater $b_j$ and $a_i$ \textless $a_{i+1}$ \textless \dots \textless $a_{n/2}$ \newline
  $(\Rightarrow)$ $a_{n/2}$ \textgreater $a_{n/2-1}$ \textgreater \dots \textgreater $a_{i+1}$ \textgreater $a_i$ \textgreater $b_j$ \newline

So count of inversion will increase by number of elements remaining in $A$ beyond i. We now will move on to $a_{i+1}$
and $b_{j+1}$

 if $a_i$ \textless $b_j$ and $b_j$ \textless $b_{j+1}$ \textless \dots \textless $b_{n/2}$  \newline
  $(\Rightarrow)$   $a_i$ \textless $b_j$ \textless $b_{j+1}$ \textless \dots \textless $b_{n/2}$ \newline

So count of inversion will increase by zero,  since index of $a_i$ is less and its value is also smaller. There will  be no more inversion
pair for $a_i$ and thus we move on to $a_{i+1}$.

Thus, we will find the total inversion count accross $A$ and $B$ by traversing the array once (
travresing completely in the worst case). \newline
Adding three inversion count will give us the total inversion count for whole array.
\end{proof}

{\bf Time Complexity:}\newline
Step 1: We split the array $A$ into 2 subarrays $A$ and $B$ of half the size.
T(n) = 2 T(n/2) + $\O(Merge)$ \newline
Step 2: To merge, we only need to traverse the halves once, and for a maximum of $n$ elements at each recursion level. \newline
So, T(n) = 2 T(n/2) + $\O(n)$ \newline
By Master theorem: time complexity is $\O(n \log n)$ \newline

\section{Solution to Question 2}

For clarity, let the elements of $A$ and $B$ be indexed as $A = \{a_1, \dots, a_n\}$ and $B = \{b_1, \dots, b_n\}$.
Consider the following algorithm $\alg$ for determining the $n$th smallest element of $A \cup B$:
\begin{enumerate}
  \item Let $i = j = \frac{n}{2}$ be indices into $A$ and $B$, respectively, which may be updated in later steps.
  \item If it is the case that $a_i < b_{j+1}$ and $b_j < a_{i+1}$, return $\max \{a_i, b_j\}$.
  \item Otherwise, if $a_i > b_{j+1}$, update $i$ and $j$ so that $i \mapsto \frac{1}{2}i$ and $j \mapsto \frac{1}{2}(j + n)$.
    If, on the other hand, $b_j > a_{i+1}$, update the indices so that $i \mapsto \frac{1}{2}(i + n)$ and $j \mapsto \frac{1}{2}j$.
    If $i$ (resp. $j$) is equal to $n$, return $a_i$ (resp. $b_j$); otherwise, return to step $2$.
\end{enumerate}
We will use the following lemma to prove the correctness of $\alg$.

\begin{lemma}\label{nth-smallest-lemma}
Suppose that $i + j = n$ and let $m = \max \{a_i, b_j\}$. Then $m$ is the $n$th smallest element of $A \cup B$ if and only if $a_i < b_{j+1}$ and $b_j < a_{i+1}$.
\end{lemma}

\begin{proof}
  $(\Rightarrow)$ Suppose that $m$ is the $n$th smallest element of $A \cup B$.
  Then $m$ is larger than exactly $n - 1$ elements of $A \cup B$.
  Where can these elements possibly be located in their lists?
  If $m = a_i$, then $m$ is greater than exactly $i - 1$ elements of $A$ (since $A$ is sorted).
  Thus $m$ must be greater than exactly $(n - 1) - (i - 1) = (i + j - 1) - (i - 1) = j$ elements of $B$.
  In fact, since $B$ is sorted, $m$ must be greater than the first $j$ elements of $B$.
  In particular, this means we have $a_{i + 1} > a_i = m > b_j$, giving us one of the desired inequalities.
  Moreover, $m > b_{j+1}$ would imply the presence of an $n$th element of $A \cup B$ which is smaller than $m$, contradicting that $m$ is the $n$th smallest element of $A \cup B$.
  Thus, we have shown $a_i = m \leq b_{j+1}$, and, since the elements are distinct, this inequality must be strict, as desired.
  A symmetrical argument yields the same result in the case that $m = b_j$, completing one direction of the proof.

  $(\Leftarrow)$ Now suppose that $a_i < b_{j+1}$ and $b_j < a_{i+1}$.
  Since $m \geq a_i$, it is greater than at least $i - 1$ elements of $A$.
  Likewise, since $m \geq b_j$, it is greater than at least $j - 1$ elements of $B$.
  In fact, since $m = \max\{a_i, b_j\}$, one of these inequalities is strict and the other is actually an equality.
  Without loss of generality (the other possible choice is dealt with by a symmetrical argument), suppose $m = a_i$ and $m > b_j$.
  Then $m$ is greater than exactly $i - 1$ elements of $A$ and at least $j$ elements of $B$, so $m$ is greater than at lest $(i - 1) + j = n - 1$ elements of $A \cup B$.
  We initially supposed that $m = a_i < b_{j+1}$, so this lower bound is actually tight.
  Since $m$ is greater than exactly $n - 1$ elements of $A \cup B$, $m$ is the $n$th smallest element of $A \cup B$.
\end{proof}

\noindent {\bf Claim 1:} $\alg$ always terminates and returns the $n$th largest element of $A \cup B$.

\begin{proof}
  Firstly, note that initially we have $i = j = \frac{n}{2}$, so $i + j = n$.
  When we update the indices, we do so by $(i, j) \mapsto \left(\frac{1}{2}i, \frac{1}{2}(j + n)\right)$.
  Since \[\frac{1}{2}i + \frac{1}{2}(j + n) = \frac{1}{2}(i + j) + \frac{1}{2}n = n\] we have that $i + j = n$ is an invariant of $\alg$.
  Finally, because $n$ is a power of $2$, each subsequent image of $i$ is a multiple of $2$, so we always have $i, j \in \N$, and in particular $i$ and $j$ are always valid as indices.
  This allows us to apply Lemma~\ref{nth-smallest-lemma} at any time during our analysis of $\alg$.

  The updating of $i$ and $j$ amounts to a binary search for the correct index configuration.
  Since the position of $j$ is completely determined by the position of $i$ (lest we violate the invariant $i + j = n$), it suffices to constrain our analysis to the trajectory of $i$ under iteration of the map that updates the indices over the course of a run.
  The value of $i$ begins at $\frac{n}{2}$ and is only ever updated by either $i \mapsto \frac{i}{2}$ or $i \mapsto \frac{1}{2}(i + n)$.
  In partiuclar, $i$ begins in the inverval $I_1 = [1, n]$, and is mapped into an interval $I_2 \in \left\{[1, \frac{n}{2}], [\frac{n}{2} + 1, n]\right\}$ after the first update.
  In general, the $k$th time it is updated, $i$ is mapped into an interval $I_{k+1} \subseteq I_k$ of size $\frac{|I_k|}{2}$.
  Since the intervals containing $i$ must be nested, and $n$ is finite, there may only be a finite number of updates in a single run of $\alg$.
  Since we must return if we cannot update $i$, $\alg$ always terminates.

  Now, we claim that if $\alg$ terminates, it returns the correct answer.
  In the case that we return in Step $2$, our answer is correct by Lemma~\ref{nth-smallest-lemma}.
  If we return in Step $3$, then $i = n$ without loss of generality.
  Since $A$ is sorted, $i = n$ implies the existence of $n - 1$ elements of $A$ which are smaller than $a_i$.
  If $b_j = b_1 < a_i$, then in the previous iteration we would have had $b_1 < a_{i+1}$, so we would have terminated in Step $2$.\footnote{This looks off-by-one at first glance, but because the lists are $1$-indexed, this final iteration maps $j = 1$ to $j = 1$ and $i = n - 1$ to $i = n$.}
  Thus we have shown there are exactly $n - 1$ elements of $A \cup B$ which are less than $a_n$, so $a_n$ must be the $n$th smallest element.
  This completes the proof of correctness.
\end{proof}

\noindent {\bf Claim 2:} $\alg$ runs in $\O(\log n)$ time.

\begin{proof}
  For each $k$, the $k$th index update halves the size of the interval $I_k$ in which the desired index lies, so there may be at most $\log_{2}n = \O(\log n)$ iterations before the algorithm terminates due to the extremal case of $i \in \{1, n\}$.
\end{proof}

\section{Solution to Question 3}

For clarity, let the elements of $A$ be indexed as $A = \{a_1, \dots, a_n\}$.
Consider the following algorithm $\alg$ for determining whether $A$ is boring or not:
\begin{enumerate}
  \item Divide the array into 2 equal parts: $A_l = \{a_1, \dots, a_{n/2}\}$ and $A_r = \{a_{n/2+1}, \dots, a_n\}$
  \item Recursively solve the two halves by calling the algorithm on $A_l$ and $A_r$.
  \item Let's say that element that make $A_l$ boring is $a_l$ and that make $A_r$ boring is $a_r$, then return $a_l$ if $a_l$=$a_r$ else
return NULL \newline
  $(\Rightarrow)$ return $a_l$ == $a_r$ ? $a_l$ : NULL.
However if any one is NULL and other is not null then return the not null element.
\item If the final answer($a_f$) is NULL then array is not boring else check for the count of $a_f$ in original array and if it is greater than
$n/2$, array is boring otherwise non-boring.
\end{enumerate}
{\bf Procedure:}\newline
CheckIfBoring(\{$a_1$, \dots, $a_n$\}) \newline
if n==1 return $a_1$ \newline
\newline
else:

 $a_l$ = checkIfBoring(\{$a_1$, \dots,$a_{n/2}$\})

 $a_r$ = checkIfBoring(\{$a_{n/2+1}$, \dots, $a_n$\})

 if $a_l$ = NULL and $a_r$ = NULL return NULL (Non-boring array)

else if $a_l$ = NULL and $a_r$ NOT NULL return $a_r$

else if $a_l$ NOT  NULL and $a_r$ = NULL return $a_l$

else return $a_l$==$a_r$ ? $a_l$ : NULL\newline
Count the occurences of returned element in the whole array and if it is greater than $\frac{n}{2}$ array is boring else non-
boring.\newline
{\bf Accuracy explanation:}\newline
We will use the following lemma to prove the correctness of $\alg$.

\begin{lemma}\label{boring-array-lemma}
If $A = \{a_1, \dots, a_n\}$ is an array, then this algorithm(through divide and conquer) will always compare a pair of consecutive elements
and ignore(or delete) them if they are non-matching. It will keep on doing this recursively until it is left with one or no element(NULL) at
which point it will count the number of occurences of final element in the original array and return if the array is boring or not.
Suppose $A = \{a_1, \dots, a_n\}$ is a boring array with element $a_f$. Now, to prove the correctness of this algorithm, it is sufficient to
prove that if $a_f$ exists then this algorithm will always return $a_f$:
\end{lemma}

\begin{proof}
If $a_f$ makes the array boring and it occurs at a total of $k$ times, then: \newline
k \textgreater $\frac{n}{2}$ \newline
Now, consider the following claims: \newline
\noindent {\bf Claim 1:} There will always be atleast one pair with both elements $a_f$
\begin{proof}
{\bf proof by contradiction:} Let us assume that there are no pair of $a_f$ in $A$. That means at the maximum $a_f$ can only
occur at every alternate index. So total count of $a_f$ = $\frac{n}{2}$ \newline
But we know that $a_f$ occurs more than $\frac{n}{2}$ times. hence, contradiction to our assumption. Thus there is atleast one
pair of $a_f$.
\end{proof}
\noindent {\bf Claim 2:} If there are x number of pairs such that both elements are equal to a value other than $a_f$
then there will be x+1 number of pairs of $a_f$.
\begin{proof}
{\bf proof by contradiction:} Let us assume that there are y pair of $a_f$ in $A$ and y \textless  x+1.  \newline
So total number of $a_f$ = 2y \newline
and total number of elements other than $a_y$ = 2x \newline
Note: we don't have to consider other elements as no other pair exists thus every other pair will be of the form ($a_f$, $~a_f$)
, in  other words, one element will be $a_f$ while other with be something other than $a_f$. \newline
So,  ratio of $a_f$ in total array = $\frac{2x}{2x+2y}$ \newline
we know that $a_f$ occurs more than n/2 times. So, \newline

$\frac{2y}{2x+2y}$  \textgreater $\frac{1}{2}$

thus 2y \textgreater x + y \newline

y \textgreater x But we assumed that y \textless  x + 1 and y is an integer which is not possible \newline
Thus, contradiciton to our assumption. \newline
Hence, there will be atleast x+1 pair of $a_f$ in $A$ if there are x pairs of elements other than $a_f$.
\end{proof}
Thus, for every element(or element pair) not equal to  $a_f$ there is atleast one $a_f$ (or one pair of $a_f$) that will cancel it out and
finally we will be left with one extra pair of $a_f$. \newline
Hence, if $a_f$ exists, then alogrithm will always return $a_f$ and no other elements.
Then we can determine the count of $a_f$ in the array and check whether  the array is boring or not.
\end{proof}

{\bf Time Complexity:}\newline
Step 1: We split the array $A$ into 2 subarrays $A_l$ and $A_r$ of half the size. We choose the majority element $a_l$ and $a_r$
and compare them. If they are same we return the element else we return NULL\newline
Note: comparison takes constant time.\newline
T(n) = 2 T(n/2) + $\O(1)$ \newline
By Master theorem: time complexity of this step is $\O(\log n)$ \newline
Step 2: We count the number of times the returned element $a_f$ occur in the array $A$ \newline
This step takes $\O(n)$ time \newline
Thus time complexity is $\O(log n)$+ $\O(n)$ \newline
= $\O(n)$


\section{Solution to Question 4}

\noindent Let $L$ and $R$ denote the left and right subtrees of $T$, respectively.
We define an algorithm $\alg$ to return the weight of a minimum vertex cover by recursively comparing the weights of minimum covers of $L$ and $R$.
To this end, $\alg$ takes an additional parameter -- a sum type encoding a Yes/No/Maybe distinction -- which determines whether it includes the root of the current tree in its cover.
Finally, we take the minimum weight over all combinations of including or excluding the root in the cover.

\begin{algorithmic}
  \Function{alg}{$T = (r, L, R)$, includeRoot}
    \If{includeRoot = Yes}
      \If{$|T| \leq 3$}
        \State \Return $w(r)$
      \Else
      \State \Return $w(r) + \alg(L, \operatorname{Maybe}) + \alg(R, \operatorname{Maybe})$
      \EndIf
    \ElsIf{includeRoot = No}
      \If{$|T| \leq 3$}
        \State \Return sum of weights of leaves of $T$
      \Else
        \State \Return $\alg(L, \operatorname{Yes}) + \alg(R, \operatorname{Yes})$
      \EndIf
    \ElsIf{includeRoot = Maybe}
      \If{$|T| \leq 3$}
        \State \Return $\min\{w(r), \textnormal{sum of weights of leaves of } T\}$
      \Else
        \State \Return $\min$ of:
        \State \hspace{7em} $W_0 = w(r) + \alg(L, \operatorname{Yes}) + \alg(R, \operatorname{No})$
        \State \hspace{7em} $W_1 = w(r) + \alg(L, \operatorname{No}) + \alg(R, \operatorname{Yes})$
        \State \hspace{7em} $W_2 = w(r) + \alg(L, \operatorname{No}) + \alg(R, \operatorname{No})$
        \State \hspace{7em} $W_3 = \alg(L, \operatorname{Yes}) + \alg(R, \operatorname{Yes})$
      \EndIf
    \EndIf
  \EndFunction
\end{algorithmic}

\noindent {\bf Claim 1:} $\alg(T, \operatorname{Maybe})$ will return the minimum weight of a cover of $T$.

\begin{proof}
  For the sake of analysis, it is convenient to talk about $\alg$ as though it computes a cover, even though it technically only computes the weight of that cover.
  We could easily modify $\alg$ to return the cover itself by storing the identity of a vertex when it includes its weight in a sum, so the convenience of identifying a vertex with its weight does not cause a problem for the argument.
  Our strategy is to show first that $\alg$ finds minimum covers in the base case of $n \leq 3$, and second that whenever $\alg$ merges the covers of two subtrees, it does so in a way that results in a minimum cover of the parent tree.
  If $|T| \leq 3$, then $\alg$ either returns the root or the two leaves.
  Both of these sets are covers of $T$, so $\alg$ returns a cover for trees where $n \leq 3$.
  Furthermore, if $n \leq 3$ and $\alg(T, \operatorname{Maybe})$ is called, $\alg$ will choose the result with minimum weight.
  Therefore, $\alg$ returns minimum covers for base-case trees.

  Now, it suffices to show that when $\alg$ merges two subtree covers, the resulting set is a minimum cover of the parent tree.
  Whenever $\alg$ is called on a tree with $n \geq 3$ and a Maybe argument, it returns one of the covers associated with $W_0, W_1, W_2,$ or $W_3$ -- whichever has the smallest weight.
  By inspection, we can see that all of these options are indeed covers.
  Therefore, $\alg$ returns a minimum cover unless there is some additional cover $C \notin \{W_0, W_1, W_2, W_3\}$, which has lesser weight.
  However, since the lower covers in the tree are fixed, the only other possibilities are $C = \{r, \operatorname{root}(L), \operatorname{root}(R)\}$ and $|C| \leq 1$ with $C \neq \{r\}$.
  In the first case, $w(C) > w(W_0)$ since $W_0 \subset C$.
  In the second case, $C$ is not actually a cover, because at least one of $(r, \operatorname{root}(L))$ and $(r, \operatorname{root}(R))$ is uncovered.
  Therefore the result of a merge event must always be a cover of minimum weight.
\end{proof}

\noindent {\bf Claim 2:} $\alg$ runs in $\O(n^2)$ time.

\begin{proof}
  Since $\alg$ has the structure of a typical divide-and-conquer algorithm, we can appeal to the Master Theorem.
  $\alg$ works by returning the minimum of four sums composed of the quantities $w(r), \alg(L, \operatorname{No}), \alg(L, \operatorname{Yes}), \alg(R, \operatorname{No}),$ and $\alg(R, \operatorname{Yes})$.
  The first, $w(r)$ takes constant time to compute.
  The remaining four involve recursive calls to $\alg$.
  Since $T$ is balanced, each of these four calls is made on a tree with size $\O(\frac{n}{2})$.
  The remaining work -- summing up the results and taking the minimum -- is $\O(1)$.
  In other words, we divide the problem into four sub-problems of size $\frac{n}{2}$ each, with constant-time work to recombine their solutions into a solution for an instance of size $n$.
  Therefore, the Master Theorem gives that since $2 > 2^0$, $\alg$ runs in $\O(n^{\log_{2}(4)}) = \O(n^2)$.
\end{proof}


Suppose we have n lines $L$ = \{ $l_1$, $l_2$, \dots, $l_n$ \} \newline
Sort the lines in increasing order of slope: \newline
let's say the new array is: \{ $l_1$, $l_2$, \dots , $l_n$\} where $l_1$ \textless $l_2$ \textless \dots \textless $l_n$ \newline
\begin{enumerate}
\item Divide the array into two halves. \newline
\item Call the $alg$ recursively on both the halves to return set of visible lines in each half \newline
\item Merge both the halves to get final set of visible lines \newline
\end{enumerate}
{\bf Merge Step:} \newline
Lets say we have two halves: \newline

$L_l$ = \{ $l_1$, $l_2$, \dots, $l_{n/2}$ \}  and $L_r$ = \{ $l_{n/2+1}$, \dots, $l_n$ \}  \newline
where slope of $l_1$ \textless $l_2$ \textless \dots \textless $l_n$

Now, for each $i^{th}$ element in $L_l$ (start from right to left) and $j^{th}$ element in $L_r$ (start from left to right) \newline
Compare intersection point of ($l_i$, $l_{i-1}$) = $P_0$ and  ($l_i$, $r_j$) = $P_1$ and do the following: \newline
\begin{enumerate}
\item If $P_1$ is to the right of $P_0$: \newline
Append $r_j$ to the right of left array, $L_l$ and increment j by 1 (move on to next element of right array $L_r$) \newline
\item If $P_1$ is to the left of $P_0$: \newline
Delete all entries to the right of i in left array $L_l$ and all entries to the left of j in right array $L_r$ \newline
In other words delete \{ $l_i$, $l_{i+1}$, \dots , $l_{n/2}$ \} and \{ $l_{n/2+1}$, \dots, $l_{j-1}$ \} \newline
append $j^{th}$ element of right array, $r_j$, to the right most of left array, $L_l$, and move on to next
element of right array (j=j+1). \newline
\end{enumerate}
The final array will give us all the lines which are visible.   \newline
{\bf Accuracy explanation:}\newline
We will use the following lemma to prove the correctness of $\alg$.

\begin{lemma}\label{visible-lines-lemma}
If $V = \{ l_1, l_2, \dots , l_{k} \}$ is a set of all the visible lines sorted in increasing order of their slopes then
the visible portion of smaller slope line will always exists to the left of visible potion of higher slope lines.
\end{lemma}

\begin{proof}
Consider three lines $l_1$ : y = $a_1$x + $b_1$ , $l_2$ : y = $a_2$x + $b_2$ 
and $l_3$ : y = $a_3$x + $b_3$  where $a_1$ \textless $a_2$ \textless $a_3$.
If lines $l_2$ and $l_3$ intersects at a point $P_{23}$ ($x_0$, $y_0$) then to the right of $P_23$
$l_3$ \textgreater $l_2$ always since it has larger slope. \newline
On the other hand, If lines $l_1$ and $l_2$ intersects at a point $P_{12}$ ($x_1$, $y_1$) then to the left of $P_{12}$
$l_1$ \textgreater $l_2$ always since it has smaller slope. \newline

So, if $x_0$ \textless $x_1$ then to its right, $l_3$ \textgreater $l_2$ while to its left $l_1$ \textgreater $l_2$ \newline
Thus, $l_2$ cannot be visible anymore.\newline
hence, visible region of smaller slope line always exists to the left of larger slope lines.
\end{proof}

Now, consider the following two claims which will establish the correctness of the algorithm: \newline
\noindent {\bf Claim 1:} If a line on the right half cuts a line on the left half such that the intersection point of two lines
lies to the left of visible portion(segment in visible part) of line in the left, then it will eclipse all the lines 
having smaller slopes than itself but larger slopes than the line under consideration. \newline
In other words, we need to prove that if If $P_1$ is to the left of $P_0$ then we should delete all lines
to the right of i(left half) and all lines to the left of j(right half)
\begin{proof}
{\bf proof by contradiction:} 
\end{proof}
\noindent {\bf Claim 2:} If a line on the right half cuts a line on the left half such that the intersection point of two lines
lies to the right of visible portion(segment in visible part) of line in the left, then it can never eclipse any line that 
exists in the left half. \newline
\begin{proof}
{\bf proof by contradiction:} 
\end{proof}

{\bf Time Complexity:)} (Same as merge sort) \newline
Step 1: We split the array $L$ into 2 subarrays $L_l$ and $L_r$ of half the size and recursively solve each of them. 
So, T(n) = 2 T(n/2) + \dots \newline
Step 2: For each recursion, the merge step traverse the array once, and in the worst ccase traverse whole of it. \newline
So, this step takes $\O(n)$ time. \newline
Thus, T(n) = 2 T(n/2) + $\O(n)$ \newline
By Master theorem: time complexity of this step is $\O(n\log n)$ \newline

\end{document}
